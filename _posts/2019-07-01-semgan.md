---
author: "ml-lab"
layout: post
title: "SemGAN for Semantic Image Generation"
slug: "SemGAN"
date: 2019-07-01 8:00:00
categories: machine-learning
image: images/semgan/semgan.png
tags: machine-learning
description: "Semantic GAN for Adversarial Semantic Image Generation"
---

<div class="blog-image-container">
<a href="/images/semgan/semgan.png"><img class="blog-image" src="/images/semgan/semgan.png"> </a>
</div>
[Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) (**GANs**) have obtained extraordinary success in the generation of **realistic** images, a domain where a **lower** pixel-level accuracy is acceptable.
However, there are scenarios (like those we face here at Zuru Tech), in which the GAN output must be **precise** at pixel level. These are cases in which the generated image should be "machine-readable" without post-processing.

The two main scenarios in which we need pixel-level accuracy are:

- Semantic Segmentation
- Semantic Image Generation

In this work, we focus on **Semantic Image Generation**, even if our architecture can be applied also to the problem of Semantic Segmentation.
Applying the standard GAN architecture to the task of (unconditional) Semantic Image Generation results in colors (and thus labels) not well defined, and a not-neat class distinction.

<div class="blog-image-container">
<figure>
<a href="/images/semgan/gan_cityscape.png"><img class="blog-image" style="width: 50%" src="/images/semgan/gan_cityscape.png"> </a>
<figcaption> Results of applying a Standard GAN architecture to the <a href="https://www.cityscapes-dataset.com/">Cityscape</a> dataset.</figcaption>
</figure>
</div>

In our paper, [Adversarial Pixel-Level Generation of Semantic Images](https://arxiv.org/abs/1906.12195), we propose a new GAN architecture, **SemGAN**, able to solve the problem of inaccurate output modeling directly the class distribution instead of the label color.

<div class="blog-image-container">
<figure>
<a href="/images/semgan/semgan_cityscape.png"><img class="blog-image" style="width: 50%" src="/images/semgan/semgan_cityscape.png"> </a>
<figcaption> <b>SemGAN</b> results. </figcaption>
</figure>
</div>

We compared our architecture with the standard GAN architecture on three datasets. Moreover, we propose an interesting application in which we let our network "dream" realistic street scenes, feeding a [Pix2Pix](https://phillipi.github.io/pix2pix/) model with the output of our generator.

<hr>

For more details, please see our [paper](https://arxiv.org/pdf/1906.12195.pdf).

This work was done by our ML&CV Team: Emanuele Ghelfi, Paolo Galeone, Federico Di Mattia, and Michele De Simoni.
