---
author: "pgaleone"
layout: "post"
title:  "3D-BoNet analysis and TensorFlow 2 implementation"
slug:   "3D-BoNet analysis and TensorFlow 2 implementation"
date:    2021-04-20 8:00:00
categories: machine-learning
image: images/3dbonet/3d-bonet-architecture.png
tags: machine-learning
---

Instance segmentation on point clouds is a challenging problem. Point clouds are unordered, sparse, non-uniform, and finding non-regulars shapes is not trivial. Moreover, the number of points in a scene can easily surpass the million, making every algorithm on point clouds very computationally expensive.
The 3D-BoNet architecture [1] has been introduced recently to tackle the problem. This network is trainable end-to-end, and it mixes the results of a bounding box prediction branch with the output of an instance segmentation branch. The article contains a walkthrough of the paper, section by section, and at the end introduces the steps we followed to convert the original implementation (in TensorFlow 1) to our own implementation in TensorFlow 2.

<div class="blog-image-container">
    <figure>
        <img class="blog-image" alt="point mask prediction branch" src="/images/3dbonet/instance-header.png">
        <figcaption>A sample extracted from the paper [1]. On the left the input point cloud, center the instance segmentation result with 3D-BoNet, right the ground truth.</figcaption>
    </figure>
</div>

## Introduction

Instance segmentation is the natural intersection between object detection and semantic segmentation. Object detection tries to identify instances within the point cloud, while semantic segmentation assigns a semantic class to every point in the cloud.
Instance segmentation aims to classify every point like semantic segmentation, but now distinct objects from the same semantic class have different labels. For this reason, an instance segmentation model should be able to reason over multiple scales: it has to recognize fine details to classify the points correctly, and, at the same time, it has to take into account objects in their entirety.

Analysis of point clouds is burdensome because point clouds are inherently unordered, unstructured, and non-uniform. The approaches to work with point clouds can be divided into three families: projection-based, discretization-based, and point-based methods. In project-based methods, the point cloud is projected to a plane, and the resulting image is analyzed with image processing techniques. Discretization-based approaches organize points in a grid of voxels summarizing the point features in voxels. This discretization allows using 3D convolutional neural networks because it imposes a regular geometric structure. The main drawback of this representation is its memory footprint, which can be very high if the scene covers a large section of space.

The 3D-BoNet approach is point-based like SGPN [2], ASIS [3], JSIS3D [4], MASC [5], 3D-BEVIS [6], but all these methods do not explicitly detect object boundaries (the bounding boxes). Furthermore, they require computationally intensive post-processing, like mean-shift clustering.

Differently from previous models, 3D-BoNet directly regresses the vertices of instance bounding boxes. Then it selects the points belonging to the instance with a binary classifier applied to the points within the bounding box. For doing this, the authors introduced a **bounding box prediction module** and a series of carefully designed loss functions to directly learn object boolean masks.

The pure 3D-BoNet method does not require post-processing, but it only works on small point clouds (usually composed of 4096 points). It is necessary to split the cloud into spatial blocks to manage more numerous point clouds. A spatial block contains the points belonging to a vertical column of space with a basis measuring a 1-meter square. The number of points in a block is constant to allow the creation of batches. For this reason, from a single spatial block, many processing blocks are created by sampling points. 

## Architecture

<div class="blog-image-container">
    <figure>
        <img class="blog-image" alt="detailed architecture" src="/images/3dbonet/3d-bonet-architecture.png">
        <figcaption>The 3D-BoNet architecture</figcaption>
    </figure>
</div>

The architecture is very similar to a traditional architecture used on images: backbone (feature extractor) and different heads for multi-task learning.

The input point cloud $$\textbf{P} \in \mathbb{R}^{N \times k_0}$$ has $$N$$ points and $$ k_0 $$ feature channels (such as the location coordinates $$\{x,y,z\}$$ and color $$\{r,g,b\}$$). The backbone network extracts **point local features** $$\textbf{F}_l \in \mathbb{R}^{N \times k}$$, where $$k$$ is the length of feature vectors.
Then, local features are aggregated in a **global feature vector** $$\textbf{F}_g \in \mathbb{R}^{1 \times k}$$.

The **Bounding Box Prediction Branch** (BBPB) takes global features $$\textbf{F}_g$$ as input and regresses a **predefined and fixed set of bounding boxes** $$B$$ and the corresponding box scores $$B_s$$.

During the training, this branch is supervised with ground truth boxes. A **bounding box association layer** associates the most similar predicted bounding box to every ground truth box. The output of the association layer is a list of indices $$\textbf{A}$$ that describes the best pairing between boxes. Paired boxes are then compared with a multi-criteria loss.
The bounding box association layer and multi-criteria loss (the content of the yellow box in the figure) are discarded for inference.

In parallel with the BBPB, predicted boxes, local features $$\textbf{F}_l$$ and global features $$\textbf{F}_g$$ are fed into the **Point Mask Prediction Branch** (PMPB).

#### The Bounding Box Prediction Branch (BBPB)

<div class="blog-image-container">
    <figure>
        <img class="blog-image" alt="bbox prediction" src="/images/3dbonet/3d-bo-net-bbox.png">
    <figcaption> The Bounding Box Predictiond Branch </figcaption>
    </figure>
</div>

The goal of the BBPB is to predict a rectangular bounding box for each instance in a single forward pass without relying on predefined spatial anchors or RPN (Region Proposal Network). A bounding box is parametrized by a pair of opposed vertices: $$ \{ [x_\text{min}, y_\text{min}, z_\text{min}], [x_\text{max}, y_\text{max}, z_\text{max}]\} $$

As for all detection tasks, the number of total instances is variable.
The BBPB gets around this problem by predicting a **fixed number of bounding boxes together with confidence scores**.
At inference time, only the boxes with a high confidence score are retained.

Since the instances have no natural order, they are predicted in random order. For this reason, it is not trivial how to link predicted bounding boxes with ground truth labels to supervise the network. **Given a set of ground truth instances, we need to determine which predicted boxes best fit them.**
This problem can be formulated as an optimal assignment and solved using an existing solver.

To supervise the network during the training, the authors introduce a **bounding box association layer** that relies on the [Hungarian algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm) to perform the association. A multi-criteria loss pushes the network to minimize the distance between paired boxes. At the same time, the loss promotes the maximization of the coverage of instance points inside predicted boxes.

#### Point Mask prediction branch

The predicted boxes and the global features extracted by the backbone, are then fed into the point mask prediction branch to predict a point-level binary mask for each instance.

Goal: classify whether each point inside of a bbox belongs to the valid instance or the background.

#### Neural architecture

The global feature vector $$\textbf{F}_g$$ is fed through 2 fully connected layers with Leaky ReLU, followed by 2 parallel FC layers.

- 1 layer outputs a 6H dimensional vector, reshaped into a $$ H \times 2 \times 3$$ tensor, where H is the **predefined and fixes number of bounding boxes**
- 1 layer outputs a H dimensional vector followed by a **sigmoid** to represent the bounding box scores.


**Bounding Box Association Layer**

Given the H predicted bboxes $$ \textbf{B} \in \mathbb{R}^{H \times 2 \times 3} $$ how to use the gt bboxes $$\bar{\textbf{B}} \in \mathbb{R}^{T \times 2 \times 3}$$ to supervise the network?

For every input point cloud, the number of gt boxes T varies  and it's different from H - very often T < H. Moreover, there's no box order for either predicted or gt boxes.

**Optimal Association Formulation**

Let $$\textbf{A}$$ be a boolean association matrix where $$\textbf{A}_{i,j} = 1 \iff \text{the} \quad i^{\text{th}}$$ predicted bbox is assigned to the $$j^{\text{th}}$$ gt box.

$$\textbf{A}$$ = the association index.

Let $$\textbf{C}$$ be the association **cost matrix** where $$\textbf{C}_{i,j}$$ represents the cost that the $$i^{th}$$ predicted box is assigned to the $$j^{\text{th}}$$ gt box.

The less the cost, the more similar are 2 boxes. Hence, the box association problem is to find the optimal assignment matrix $$\textbf{A}$$ with the **minimal cost overall**:

$$ A = \text{argmin}_{A} \sum_{i=1}^{H}{\sum_{j=1}^{T}{C_{i,j}A_{i,j}}} $$ subject to $$ \sum_{i=1}^{H}{A_{i,j}} = 1, \sum_{j=1}^{T}{A_{i,j}} \le 1, j \in {1..T}, i \in {1..H} $$

To solve the above optimal association problem, the existing **Hungarian algorithm** is applied.

#### Association Matrix Calculation

To evaluate the similarity between the $$i^\text{th}$$ predicted bbox and the $$j^\text{th}$$ gt box, the Euclidean distance between 2 pairs of min-max vertices is not enough, because we need the predicted bbox to contain as many valid points as possible since the points clouds are usually sparse and distributed non-uniformly in 3D space.

Therefore, also the **coverage** of valid points should be included the calculate the cost matrix C. The final association cost is the sum of 3 costs.

1. **Euclidean Distance between Vertices**. $$ C^{ed}_{i,j} = \frac{1}{6}\sum{(B_i - \bar{B}_j)^2} $$
2. **Soft IoU on Points**. For every point of the point cloud, we need to know if it falls inside a  predicted bbox or not.
    It's possible to obtain a **hard-binary vector** $$\bar{q}_j \in R^N$$ to represents whether each point is inside of the box or not, where every $$\bar{q}_i%$$ is 1 or 0.

    But we also need a differentiable value (wrt predicted boxes) that associates to every point of the point cloud a probability value, that this point is inside a predicted bounding box.

    The vector $$q$$ is called **point-in-pred-box-probability** and it's obtained applying the following algorithm.

    **Algorithm for calculating point-in-pred-box probability**

    Given
    - H, the number of predicted boxes B
    - N, the number of points in the point cloud P
    - $$\theta_1$$ and $$\theta_2$$ hyperparameters (value 100 and 20 respectively).

    <div class="blog-image-container">
        <figure>
            <img class="blog-image" alt="Algorithm" src="/images/3dbonet/algorithm1.png" style="width:50%">
            <figcaption>The algorithm for calculating point-in-pred-box probability</figcaption>
        </figure>
    </div>

    The deeper the corresponding point is inside of the box, the higher the value.

    The SIoU (Soft IoU) cost between the $$i^\text{th}$$ predicted box and the $$j^\text{th}$$ gt box is defined as

    $$ C_{i,j}^{\text{sIoU}} = \frac{-\sum_{n=1}^{N}{(q_i^n \cdot \bar{q}^n_j)}}{\sum_{n=1}^N{q_i^n}+\sum_{n=1}^{N}{\bar{q}^n_n} - \sum_{n=1}^N{(q_i^n \cdot \bar{q}_j^n)}} $$

    where $$q^n_i$$ and $$\bar{q}^n_j$$ are the $$n^\text{th}$$ values of $$q_i$$ and $$\bar{q}_j$$.

3. **Cross-Entropy Score**. It's useful to consider also the cross-entropy score between $$q_i$$ and $$\bar{q}_j$$. Being different from the sIoU cost with prefers tighter boxes, this score represents how **confident** a predicted bounding box is able to include valid points as many as possible. This score prefers larger and more inclusive boxes (this means that it counter balances the sIoU) and it's formally defined as:

$$ C^\text{ces}_{i,j} = - \frac{1}{N}\sum_{n=1}^N{[\bar{q}_j^n \log q_i^n + (1 - \bar{q}_j^n)\log(1-q_i^n)]} $$


The final association cost between the $$i^\text{th}$$ and the $$j^\text{th}$$ gt box is defined as:

$$ C_{i,j} = C^{ed}_{i,j} + C^{\text{sIoU}}_{i,j} + C^{\text{ces}}_{i,j} $$

Where the Euclidean criterion guarantees the geometric boundaries, and the sIoU and the ces criteria maximize the coverage of valid points and overcome the non uniformity.


#### Loss Functions

NOTE: after the bbox association layer, both predicted boxes and scores are reordered using the association index A, such that the first predicted boxes and the scores are wall paired with the T gt boxes.

**Multi-criteria Loss for Box prediction**.  We want to minimize the association cost previously defined, e.g.

$$ \mathcal{l_\text{bbox}} = \frac{1}{T} \sum_{t=1}^{T}{C^{ed}_{t,t} + C^{\text{sIoU}}_{t,t} + C^{\text{ces}}_{t,t}} $$

where all the terms are the costs of the $$t^\text{th}$$ paired boxes. NOTE: we only minimize the cost of T paired boxes, the remaining H- T are ignored.

**Loss for Box Score Prediction**. The predicted box scores aim to indicate the validity of the corresponding predicted boxes. After being reordered, the GT scores for the first T scores are all 1 and 0 for all the remaining H-T scores. We can thus use the cross entropy loss for this binary classification task:


$$ \mathcal{l_\text{bbs}} = -\frac{1}{H} \left[ \sum_{t=1}^{T}{B^t_s} + \sum_{t=T+1}^{H}{\log(1- B^t_s)} \right] $$

where $$B_s^t$$ is the $$t^\text{th}$$ predicted score after being associated. This loss rewards correctly predicted bounding boxes and implicitly penalizes the cases where multiple similar boxes are regressed for a single instance.

### Point mask prediction branch

<div class="blog-image-container">
    <figure>
        <img class="blog-image" alt="point mask prediction branch" src="/images/3dbonet/3d-pointmask-prediction.png">
        <figcaption>Point mask prediction branch architecture</figcaption>
    </figure>
</div>

Given the predicted boxes B, the learn point features $$F_l$$ and the global features $$F_g$$, the point mask prediction branch processes each bbox individually with shared neural layers.

**Neural layers**: The point and global features are compressed to a 256-dimensional vector trough FC layers, before being concatenated and compressed to a 128-dimensional mixed point feature $$\tilde{F}_l$$.

For the $$i^\text{th}$$ predicted bbox $$B_i$$ the estimated vertices and score are fused with features $$\tilde{F}_l$$ through concatenation, producing a **box-aware features** $$\hat{F}_l$$. These features are then fed through shared layers, predicting a point level binary mask, denoted as $$M_i$$. Sigmoid is then applied as last activation function.

**Loss function**

The predicted instance masks M are similarly associated with the gt masks according to the association index A. Due to imbalance of instance and background point they use **focal loss** with default hyper-parameters instead of the standard cross-entropy. Only the valid T paired masks are used for the loss $$\mathcal{l_\text{pmask}}$$.

## End-to-End implementation & semantic segmentation loss

Backbone of choice: PointNet++.

For semantic segmentation, pointwise softmax cross-entropy loss function is used $$\mathcal{l_\text{sem}}$$. Given an input point cloud P, all the branches are linked and end-to-end trained using a single combined multi-task loss:

$$ \mathcal{l_\text{all}} = \mathcal{l_\text{sem}} + \mathcal{l_\text{bbox}} + \mathcal{l_\text{bbs}} + \mathcal{l_\text{pmask}} $$


Optimizer Adam. lr: 5e-4, divided by 2 every 20 epochs.

---

[1] [Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds](https://arxiv.org/abs/1906.01140)
[2] [SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation](https://arxiv.org/abs/1711.08588)
[3] [Associatively Segmenting Instances and Semantics in Point Clouds](https://arxiv.org/abs/1902.09852)
[4] [JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields](https://arxiv.org/abs/1904.00699) 
[5] [MASC: Multi-scale Affinity with Sparse Convolution for 3D Instance Segmentation](https://arxiv.org/abs/1902.04478)
[6] [3D-BEVIS: Birds-Eye-View Instance Segmentation](https://arxiv.org/abs/1904.02199)